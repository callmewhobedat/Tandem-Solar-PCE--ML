{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# Learning NumPy, SciPy, Pandas, and Gradient Descent Intuitively\n\n## Welcome! üöÄ\nThis notebook is designed to help you understand fundamental Python libraries and machine learning concepts through **interactive examples** and **visual explanations**.\n\n### What You'll Learn:\n1. **NumPy** - Working with numerical arrays efficiently\n2. **Pandas** - Manipulating and analyzing tabular data\n3. **SciPy** - Advanced scientific computing\n4. **Gradient Descent** - The core optimization algorithm in machine learning\n\n---\n\n## Part 1: NumPy - The Foundation of Scientific Computing üî¢" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.optimize import minimize_scalar, minimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set styling for better plots\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['font.size'] = 10\n\nprint('‚úì All libraries imported successfully!')" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.1 Creating Arrays\n\nNumPy arrays are like Python lists, but **much faster** and **more efficient** for numerical computations." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Different ways to create arrays\n\n# From a Python list\narr1 = np.array([1, 2, 3, 4, 5])\nprint("Array from list:", arr1)\n\n# Zeros, ones, and range\nzeros = np.zeros(5)\nones = np.ones(5)\nrange_arr = np.arange(0, 10, 2)  # Start, stop, step\nprint("\nZeros:", zeros)\nprint("Ones:", ones)\nprint("Range (0 to 10, step 2):", range_arr)\n\n# Linearly spaced values\nlinspace = np.linspace(0, 100, 5)  # 5 values from 0 to 100\nprint("\nLinespace (5 values from 0 to 100):", linspace)\n\n# Random numbers\nrandom_arr = np.random.rand(5)  # 5 random numbers between 0 and 1\nprint("\nRandom array:", random_arr)" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.2 Array Operations and Broadcasting\n\nNumPy's real power comes from **element-wise operations** without explicit loops." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Element-wise operations\na = np.array([1, 2, 3, 4])\nb = np.array([10, 20, 30, 40])\n\nprint("Array a:", a)\nprint("Array b:", b)\nprint("\na + b =", a + b)\nprint("a * b =", a * b)\nprint("b / a =", b / a)\nprint("a ** 2 =", a ** 2)  # Element-wise power\n\n# Broadcasting: operations between arrays of different shapes\nprint("\n--- Broadcasting Example ---")\nc = np.array([1, 2, 3, 4])\nscalar = 10\nprint(f"Array c: {c}")\nprint(f"Scalar: {scalar}")\nprint(f"c + scalar = {c + scalar}")\nprint(f"c * scalar = {c * scalar}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.3 Mathematical and Statistical Functions" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "data = np.array([10, 20, 30, 40, 50])\n\nprint("Data:", data)\nprint(f"Sum: {np.sum(data)}")\nprint(f"Mean: {np.mean(data)}")\nprint(f"Std Dev: {np.std(data)}")\nprint(f"Min: {np.min(data)}")\nprint(f"Max: {np.max(data)}")\n\n# Trigonometric and exponential functions\nangles = np.array([0, np.pi/4, np.pi/2, np.pi])\nprint(f"\nAngles (in radians): {angles}")\nprint(f"sin(angles): {np.sin(angles)}")\nprint(f"cos(angles): {np.cos(angles)}")\n\n# Exponential\nvals = np.array([0, 1, 2])\nprint(f"\nValues: {vals}")\nprint(f"e^values: {np.exp(vals)}")\nprint(f"log(values+1): {np.log(vals + 1)}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.4 Reshaping and Indexing\n\nManipulating array shapes is crucial for data processing." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Creating a 1D array and reshaping it\nflat = np.arange(12)\nprint("Flat array:", flat)\n\n# Reshape to 2D (3x4 matrix)\nmatrix = flat.reshape(3, 4)\nprint("\nReshaped to 3x4:\n", matrix)\n\n# Reshape to 3D (2x2x3)\ntensor = flat.reshape(2, 2, 3)\nprint("\nReshaped to 2x2x3:\n", tensor)\n\n# Indexing and slicing\nprint("\n--- Indexing ---")\narr = np.array([10, 20, 30, 40, 50])\nprint(f"Array: {arr}")\nprint(f"First element (index 0): {arr[0]}")\nprint(f"Last element (index -1): {arr[-1]}")\nprint(f"Elements from index 1 to 3: {arr[1:4]}")\nprint(f"Every 2nd element: {arr[::2]}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 1.5 Boolean Indexing (Filtering)\n\nThis is powerful for selecting data based on conditions." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "temperatures = np.array([15, 22, 28, 19, 31, 25, 30])\nprint(f"Temperatures: {temperatures}")\n\n# Find hot days (>= 28¬∞C)\nhot_days = temperatures >= 28\nprint(f"\nTemperatures >= 28: {hot_days}")\nprint(f"Hot temperatures: {temperatures[hot_days]}")\n\n# Multiple conditions\nwarm_days = (temperatures >= 20) & (temperatures <= 25)\nprint(f"\nTemperatures between 20 and 25: {temperatures[warm_days]}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## Part 2: Pandas - Working with Tabular Data üìä" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 2.1 Series and DataFrames\n\nA **Series** is like a column in Excel. A **DataFrame** is like an entire spreadsheet." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Creating a Series\nprices = pd.Series([100, 200, 150, 300], index=['Apple', 'Banana', 'Cherry', 'Date'])\nprint("Price Series:")\nprint(prices)\nprint(f"\nType: {type(prices)}")\nprint(f"Mean price: {prices.mean()}")" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Creating a DataFrame\ndata = {\n    'Student': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Math': [85, 90, 78, 92, 88],\n    'Science': [88, 85, 92, 90, 95],\n    'English': [90, 88, 85, 89, 92]\n}\n\ndf = pd.DataFrame(data)\nprint("Student Grades DataFrame:")\nprint(df)\nprint(f"\nShape: {df.shape} (5 rows, 4 columns)")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 2.2 Data Selection and Filtering" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "print("Original DataFrame:")\nprint(df)\n\n# Select a single column\nprint("\n--- Math scores ---")\nprint(df['Math'])\n\n# Select multiple columns\nprint("\n--- Math and Science ---")\nprint(df[['Math', 'Science']])\n\n# Select rows by index\nprint("\n--- First 2 rows ---")\nprint(df.iloc[:2])\n\n# Filter by condition\nprint("\n--- Students with Math >= 90 ---")\nprint(df[df['Math'] >= 90])" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 2.3 Aggregation and Grouping" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Calculate statistics\nprint("Summary statistics:")\nprint(df.describe())\n\n# Calculate mean for each subject\nprint("\nMean score by subject:")\nprint(df[['Math', 'Science', 'English']].mean())\n\n# Add a new column (average score)\ndf['Average'] = df[['Math', 'Science', 'English']].mean(axis=1)\nprint("\nDataFrame with Average score:")\nprint(df)" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## Part 3: SciPy - Advanced Scientific Computing üî¨" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 3.1 Basic Optimization with SciPy\n\nSciPy provides tools to find optimal values (minimum/maximum) of functions." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Define a simple function to minimize\ndef quadratic_function(x):\n    \"\"\"A simple parabola: f(x) = (x-3)^2 + 2\"\"\"\n    return (x - 3) ** 2 + 2\n\n# Plot the function\nx = np.linspace(-1, 7, 100)\ny = quadratic_function(x)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x, y, 'b-', linewidth=2, label='f(x) = (x-3)¬≤ + 2')\n\n# Use SciPy to find the minimum\nresult = minimize_scalar(quadratic_function)\nmin_x = result.x\nmin_y = result.fun\n\nplt.plot(min_x, min_y, 'ro', markersize=10, label=f'Minimum at x={min_x:.2f}, y={min_y:.2f}')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Finding the Minimum of a Function using SciPy')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f"Minimum found at x = {min_x:.4f}")\nprint(f"Minimum value f(x) = {min_y:.4f}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 3.2 Curve Fitting with SciPy" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "from scipy.optimize import curve_fit\n\n# Generate some noisy data\nnp.random.seed(42)\nx_data = np.linspace(0, 10, 30)\ny_data = 2 * x_data + 1 + np.random.normal(0, 2, 30)  # y = 2x + 1 + noise\n\n# Define a linear model\ndef linear_model(x, slope, intercept):\n    return slope * x + intercept\n\n# Fit the model\nparams, _ = curve_fit(linear_model, x_data, y_data)\nfitted_slope, fitted_intercept = params\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.scatter(x_data, y_data, alpha=0.6, label='Data points')\nplt.plot(x_data, linear_model(x_data, *params), 'r-', linewidth=2, \n         label=f'Fitted line: y = {fitted_slope:.2f}x + {fitted_intercept:.2f}')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Curve Fitting with SciPy')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f"Fitted parameters:")\nprint(f"  Slope: {fitted_slope:.4f}")\nprint(f"  Intercept: {fitted_intercept:.4f}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## Part 4: Gradient Descent - The Heart of Machine Learning ‚≠ê\n\n### What is Gradient Descent?\n\nImagine you're **hiking down a foggy mountain**. You can't see the bottom, but you can feel the ground beneath your feet.\n- The **slope** of the ground tells you the **direction** to go (the **gradient**).\n- You take **small steps** in the direction of steepest descent (the **negative gradient**).\n- Eventually, you reach the **bottom** (the **minimum**).\n\nThis is exactly what **gradient descent** does to find the best parameters for a machine learning model!" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 4.1 Understanding Gradient Descent with a 1D Example" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Define a loss function (what we're trying to minimize)\ndef loss_function_1d(x):\n    return (x - 5) ** 2 + 3\n\n# Define the gradient (derivative)\ndef gradient_1d(x):\n    return 2 * (x - 5)\n\n# Gradient Descent Implementation\ndef gradient_descent_1d(start_x, learning_rate, iterations):\n    x = start_x\n    history = [x]\n    loss_history = [loss_function_1d(x)]\n    \n    for i in range(iterations):\n        grad = gradient_1d(x)\n        x = x - learning_rate * grad  # Update: move opposite to gradient\n        history.append(x)\n        loss_history.append(loss_function_1d(x))\n    \n    return x, history, loss_history\n\n# Run gradient descent\nstart_x = 0\nlearning_rate = 0.1\niterations = 50\n\nfinal_x, x_history, loss_history = gradient_descent_1d(start_x, learning_rate, iterations)\n\nprint(f"Starting point: x = {start_x}")\nprint(f"Learning rate: {learning_rate}")\nprint(f"Iterations: {iterations}")\nprint(f"\nFinal x: {final_x:.4f}")\nprint(f"Final loss: {loss_history[-1]:.4f}")\nprint(f"Optimal x should be: 5.0")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 4.2 Visualizing Gradient Descent on a 1D Function" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Plot the function and gradient descent path\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left plot: Function and descent path\nx_range = np.linspace(-2, 12, 200)\ny_range = loss_function_1d(x_range)\n\nax1.plot(x_range, y_range, 'b-', linewidth=2, label='Loss function')\nax1.plot(x_history, loss_history, 'ro-', markersize=4, alpha=0.7, label='Gradient descent path')\nax1.plot(x_history[0], loss_history[0], 'go', markersize=10, label='Start')\nax1.plot(x_history[-1], loss_history[-1], 'r*', markersize=15, label='End')\nax1.plot(5, 3, 'k*', markersize=20, label='Global minimum')\nax1.set_xlabel('x')\nax1.set_ylabel('Loss')\nax1.set_title('Gradient Descent Path (1D)')\nax1.legend()\nx1.grid(True, alpha=0.3)\n\n# Right plot: Loss over iterations\nax2.plot(loss_history, 'b-', linewidth=2)\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Loss')\nax2.set_title('Loss Convergence')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 4.3 Impact of Learning Rate\n\nThe **learning rate** controls how big each step is. Too small = slow, too large = unstable!" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Compare different learning rates\nlearning_rates = [0.01, 0.05, 0.1, 0.15, 0.2]\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\naxes = axes.flatten()\n\nfor idx, lr in enumerate(learning_rates):\n    try:\n        final_x, x_hist, loss_hist = gradient_descent_1d(0, lr, 50)\n        \n        x_range = np.linspace(-2, 12, 200)\n        y_range = loss_function_1d(x_range)\n        \n        axes[idx].plot(x_range, y_range, 'b-', linewidth=2)\n        axes[idx].plot(x_hist, loss_hist, 'ro-', markersize=3, alpha=0.6)\n        axes[idx].plot(5, 3, 'g*', markersize=15)\n        axes[idx].set_title(f'Learning Rate = {lr}')\n        axes[idx].set_xlabel('x')\n        axes[idx].set_ylabel('Loss')\n        axes[idx].grid(True, alpha=0.3)\n    except:\n        axes[idx].text(0.5, 0.5, 'Diverged!\n(Learning rate too high)', \n                       ha='center', va='center', fontsize=12, color='red')\n        axes[idx].set_title(f'Learning Rate = {lr}')\n        axes[idx].axis('off')\n\n# Hide the last subplot\naxes[5].axis('off')\n\nplt.suptitle('Impact of Learning Rate on Gradient Descent', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint("üí° Observations:")\nprint("  - Too small (0.01): Converges slowly")\nprint("  - Just right (0.1): Converges smoothly")\nprint("  - Too large (0.2+): May diverge or oscillate")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 4.4 2D Gradient Descent (Multiple Parameters)\n\nMost real ML problems have many parameters. Let's see what happens with 2 parameters." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Define a 2D loss function\ndef loss_2d(params):\n    w1, w2 = params\n    return (w1 - 3) ** 2 + (w2 + 2) ** 2\n\n# Define the gradient (partial derivatives)\ndef gradient_2d(params):\n    w1, w2 = params\n    dL_dw1 = 2 * (w1 - 3)\n    dL_dw2 = 2 * (w2 + 2)\n    return np.array([dL_dw1, dL_dw2])\n\n# Gradient Descent for 2D\ndef gradient_descent_2d(start_params, learning_rate, iterations):\n    params = start_params.copy()\n    history = [params.copy()]\n    loss_history = [loss_2d(params)]\n    \n    for i in range(iterations):\n        grad = gradient_2d(params)\n        params = params - learning_rate * grad\n        history.append(params.copy())\n        loss_history.append(loss_2d(params))\n    \n    return params, np.array(history), loss_history\n\n# Run 2D gradient descent\nstart_params = np.array([-5, 5])\nfinal_params, params_history, loss_2d_history = gradient_descent_2d(start_params, 0.1, 50)\n\nprint(f"Starting parameters: w1={start_params[0]}, w2={start_params[1]}")\nprint(f"Final parameters: w1={final_params[0]:.4f}, w2={final_params[1]:.4f}")\nprint(f"Optimal should be: w1=3, w2=-2")\nprint(f"\nFinal loss: {loss_2d_history[-1]:.4f}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 4.5 Visualizing 2D Gradient Descent with Contour Plot" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Create a contour plot\nw1_range = np.linspace(-6, 6, 100)\nw2_range = np.linspace(-5, 7, 100)\nW1, W2 = np.meshgrid(w1_range, w2_range)\nZ = (W1 - 3) ** 2 + (W2 + 2) ** 2\n\nplt.figure(figsize=(10, 8))\ncontour = plt.contour(W1, W2, Z, levels=20, cmap='viridis', alpha=0.6)\nplt.clabel(contour, inline=True, fontsize=8)\n\n# Plot the gradient descent path\nplt.plot(params_history[:, 0], params_history[:, 1], 'ro-', markersize=5, alpha=0.7, label='GD path')\nplt.plot(params_history[0, 0], params_history[0, 1], 'go', markersize=10, label='Start')\nplt.plot(params_history[-1, 0], params_history[-1, 1], 'r*', markersize=15, label='End')\nplt.plot(3, -2, 'y*', markersize=20, label='Optimal (3, -2)')\n\nplt.xlabel('w1')\nplt.ylabel('w2')\nplt.title('2D Gradient Descent: Contour Plot')\nplt.colorbar(contour, label='Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## Part 5: Practical Application - Linear Regression with Gradient Descent üéØ" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 5.1 Creating Synthetic Data" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Generate synthetic data: y = 2.5*x + 1 + noise\nnp.random.seed(42)\nn_samples = 50\nX = np.random.randn(n_samples) * 10 + 50  # Features\ny = 2.5 * X + 1 + np.random.randn(n_samples) * 10  # Target: y = 2.5*x + 1 + noise\n\nplt.figure(figsize=(10, 5))\nplt.scatter(X, y, alpha=0.6, s=50)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Synthetic Data for Linear Regression')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f"Generated {n_samples} data points")\nprint(f"True relationship: y = 2.5*X + 1 + noise")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 5.2 Implementing Linear Regression from Scratch" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "class LinearRegressionGD:\n    \"\"\"Linear Regression using Gradient Descent\"\"\"\n    \n    def __init__(self, learning_rate=0.01, iterations=1000):\n        self.learning_rate = learning_rate\n        self.iterations = iterations\n        self.slope = None\n        self.intercept = None\n        self.loss_history = []\n    \n    def fit(self, X, y):\n        \"\"\"Fit the model using gradient descent\"\"\"\n        n = len(X)\n        \n        # Initialize parameters\n        self.slope = 0.0\n        self.intercept = 0.0\n        \n        for iteration in range(self.iterations):\n            # Predictions\n            y_pred = self.slope * X + self.intercept\n            \n            # Errors\n            errors = y - y_pred\n            \n            # Mean Squared Error (MSE)\n            mse = np.mean(errors ** 2)\n            self.loss_history.append(mse)\n            \n            # Gradients\n            dL_dslope = -2 * np.mean(X * errors)\n            dL_dintercept = -2 * np.mean(errors)\n            \n            # Update parameters\n            self.slope -= self.learning_rate * dL_dslope\n            self.intercept -= self.learning_rate * dL_dintercept\n        \n        return self\n    \n    def predict(self, X):\n        \"\"\"Make predictions\"\"\"\n        return self.slope * X + self.intercept\n\n# Train the model\nmodel = LinearRegressionGD(learning_rate=0.0001, iterations=1000)\nmodel.fit(X, y)\n\nprint(f"Learned parameters:")\nprint(f"  Slope: {model.slope:.4f} (true value: 2.5)")\nprint(f"  Intercept: {model.intercept:.4f} (true value: 1.0)")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 5.3 Visualizing Training Progress" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Data and fitted line\ny_pred = model.predict(X)\nax1.scatter(X, y, alpha=0.6, s=50, label='Data')\nax1.plot(X, y_pred, 'r-', linewidth=2, label=f'Fitted line: y = {model.slope:.2f}*x + {model.intercept:.2f}')\nax1.set_xlabel('X')\nax1.set_ylabel('y')\nax1.set_title('Linear Regression Result')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Right: Loss convergence\nax2.plot(model.loss_history, 'b-', linewidth=2)\nax2.set_xlabel('Iteration')\nax2.set_ylabel('Loss (MSE)')\nax2.set_title('Training Loss Over Iterations')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate R-squared\nss_res = np.sum((y - y_pred) ** 2)\nss_tot = np.sum((y - np.mean(y)) ** 2)\nr_squared = 1 - (ss_res / ss_tot)\nprint(f"\nModel Performance:")\nprint(f"  R¬≤ Score: {r_squared:.4f}")\nprint(f"  Final Loss: {model.loss_history[-1]:.4f}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## Part 6: Common Pitfalls and Best Practices ‚ö†Ô∏è" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 6.1 Problem 1: Not Scaling Data" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Unscaled data with very different ranges\nX_unscaled = np.array([[100], [200], [300], [400], [500]])  # Large range\ny_unscaled = np.array([[0.1], [0.2], [0.3], [0.4], [0.5]])  # Small range\n\n# Scaled data (normalized to 0-1)\nX_scaled = (X_unscaled - X_unscaled.min()) / (X_unscaled.max() - X_unscaled.min())\ny_scaled = (y_unscaled - y_unscaled.min()) / (y_unscaled.max() - y_unscaled.min())\n\nprint("Unscaled X range:", X_unscaled.min(), "to", X_unscaled.max())\nprint("Unscaled y range:", y_unscaled.min(), "to", y_unscaled.max())\nprint("\nScaled X range:", X_scaled.min(), "to", X_scaled.max())\nprint("Scaled y range:", y_scaled.min(), "to", y_scaled.max())\n\nprint("\nüí° Tip: Scaling helps gradient descent converge faster!")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 6.2 Problem 2: Choosing the Wrong Learning Rate" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Function: y = x^4 - 3x^2 + 2\ndef complex_loss(x):\n    return x**4 - 3*x**2 + 2\n\ndef complex_gradient(x):\n    return 4*x**3 - 6*x\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nlearning_rates = [0.001, 0.05, 0.3]\ntitles = ['Too Small (0.001)', 'Just Right (0.05)', 'Too Large (0.3)']\n\nfor idx, (lr, title) in enumerate(zip(learning_rates, titles)):\n    x = 2.0\n    history = [x]\n    loss_hist = [complex_loss(x)]\n    \n    for _ in range(100):\n        x = x - lr * complex_gradient(x)\n        history.append(x)\n        loss_hist.append(complex_loss(x))\n    \n    x_range = np.linspace(-2.5, 2.5, 200)\n    y_range = complex_loss(x_range)\n    \n    axes[idx].plot(x_range, y_range, 'b-', linewidth=2)\n    axes[idx].plot(history, loss_hist, 'ro-', markersize=3, alpha=0.6)\n    axes[idx].set_title(title)\n    axes[idx].set_xlabel('x')\n    axes[idx].set_ylabel('Loss')\n    axes[idx].grid(True, alpha=0.3)\n\nplt.suptitle('Effect of Learning Rate on Convergence', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 6.3 Problem 3: Overfitting from Too Many Iterations" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Generate training and test data\nnnp.random.seed(42)\nn_train = 30\nX_train = np.random.randn(n_train) * 10\ny_train = 2 * X_train + 1 + np.random.randn(n_train) * 8\n\nn_test = 20\nX_test = np.random.randn(n_test) * 10\ny_test = 2 * X_test + 1 + np.random.randn(n_test) * 8\n\n# Train model with many iterations\nmodel_many_iter = LinearRegressionGD(learning_rate=0.0001, iterations=5000)\nmodel_many_iter.fit(X_train, y_train)\n\n# Calculate train and test loss\ntrain_pred = model_many_iter.predict(X_train)\ntest_pred = model_many_iter.predict(X_test)\ntrain_loss = np.mean((y_train - train_pred)**2)\ntest_loss = np.mean((y_test - test_pred)**2)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Training history\nax1.semilogy(model_many_iter.loss_history)\nax1.set_xlabel('Iteration')\nax1.set_ylabel('Training Loss (log scale)')\nax1.set_title('Loss Over Many Iterations')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Train vs test loss\ncategories = ['Train Loss', 'Test Loss']\nlosses = [train_loss, test_loss]\nax2.bar(categories, losses, color=['green', 'red'], alpha=0.7)\nax2.set_ylabel('Loss')\nax2.set_title('Train vs Test Loss (Overfitting Check)')\nfor i, v in enumerate(losses):\n    ax2.text(i, v + 0.5, f'{v:.2f}', ha='center')\n\nplt.tight_layout()\nplt.show()\n\nprint(f"Train Loss: {train_loss:.4f}")\nprint(f"Test Loss: {test_loss:.4f}")\nif test_loss > train_loss * 1.5:\n    print("‚ö†Ô∏è Test loss is much higher than train loss - possible overfitting!\")\nelse:\n    print("‚úì Train and test losses are reasonable")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## Part 7: Hands-On Exercises üèãÔ∏è" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### Exercise 1: NumPy Array Operations\n\n**Task:** Given an array of temperatures in Celsius, convert them to Fahrenheit and find statistics.\n\nFormula: F = C √ó 9/5 + 32" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Temperature data in Celsius\ncelsius = np.array([0, 10, 20, 25, 30, 35, 40])\n\n# TODO: Convert to Fahrenheit\nfahrenheit = celsius * 9/5 + 32\n\nprint("Celsius:", celsius)\nprint("Fahrenheit:", fahrenheit)\nprint(f"\nMean temperature (F): {np.mean(fahrenheit):.2f}")\nprint(f"Max temperature (F): {np.max(fahrenheit):.2f}")\nprint(f"Std Dev (F): {np.std(fahrenheit):.2f}")" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### Exercise 2: Pandas Data Manipulation\n\n**Task:** Analyze a simple dataset of sales data" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Sales data\nsales_data = {\n    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B'],\n    'Quantity': [10, 15, 20, 12, 18, 22, 11, 19],\n    'Price': [100, 200, 150, 100, 200, 150, 100, 200]\n}\n\ndf_sales = pd.DataFrame(sales_data)\nprint("Sales Data:")\nprint(df_sales)\n\n# TODO: Calculate Revenue (Quantity * Price)\ndf_sales['Revenue'] = df_sales['Quantity'] * df_sales['Price']\n\nprint("\nWith Revenue:")\nprint(df_sales)\n\n# TODO: Group by product and sum revenue\nproduct_revenue = df_sales.groupby('Product')['Revenue'].sum()\nprint("\nTotal Revenue by Product:")\nprint(product_revenue)" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### Exercise 3: Gradient Descent Optimization\n\n**Task:** Minimize the function f(x) = x^3 - 6x^2 + 9x using gradient descent" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Define the function\ndef f(x):\n    return x**3 - 6*x**2 + 9*x\n\n# Define the gradient\ndef df(x):\n    return 3*x**2 - 12*x + 9\n\n# Implement gradient descent\nx = 5.0  # Starting point\nlearning_rate = 0.01\niterations = 100\nhistory = [x]\n\nfor i in range(iterations):\n    gradient = df(x)\n    x = x - learning_rate * gradient\nhistory.append(x)\n\nprint(f"Starting point: x = 5.0")\nprint(f"Final x: {x:.4f}")\nprint(f"Final f(x): {f(x):.4f}")\n\n# Visualize\nx_range = np.linspace(0, 5, 200)\ny_range = f(x_range)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x_range, y_range, 'b-', linewidth=2, label='f(x) = x¬≥ - 6x¬≤ + 9x')\nplt.plot(history, [f(h) for h in history], 'ro-', markersize=3, alpha=0.6, label='GD path')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Minimizing f(x) = x¬≥ - 6x¬≤ + 9x with Gradient Descent')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## üéì Summary and Key Takeaways\n\n### NumPy\n- ‚úÖ Fast, efficient numerical computing with arrays\n- ‚úÖ Broadcasting for operations on different-shaped arrays\n- ‚úÖ Rich set of mathematical functions\n\n### Pandas\n- ‚úÖ Tabular data manipulation (like Excel in Python)\n- ‚úÖ Easy filtering, grouping, and aggregation\n- ‚úÖ Handles missing data gracefully\n\n### SciPy\n- ‚úÖ Advanced scientific computing (optimization, integration, statistics)\n- ‚úÖ Built on top of NumPy\n- ‚úÖ Ready-made algorithms for complex problems\n\n### Gradient Descent\n- ‚úÖ Iterative algorithm to find function minima\n- ‚úÖ Learning rate is crucial: too small = slow, too large = unstable\n- ‚úÖ Used everywhere in machine learning\n- ‚úÖ Multiple variations: SGD, Adam, RMSprop, etc.\n\n### Best Practices\n1. **Scale your data** before using gradient descent\n2. **Choose learning rate carefully** (0.001 - 0.1 is often a good start)\n3. **Monitor both training and validation loss** to avoid overfitting\n4. **Visualize your data and results** to gain intuition\n5. **Start simple** and gradually add complexity\n\n---\n\n## üìö Additional Resources\n\n- **NumPy:** https://numpy.org/doc/stable/\n- **Pandas:** https://pandas.pydata.org/docs/\n- **SciPy:** https://docs.scipy.org/doc/scipy/reference/\n- **Gradient Descent:** https://cs231n.github.io/optimization-1/\n- **3Blue1Brown ML Videos:** https://www.youtube.com/c/3Blue1Brown\n\nHappy Learning! üöÄ" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "---\n\n## üí¨ Questions?\n\nFeel free to reach out or open an issue if you have questions about any of these concepts!" ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "name": "python", "version": "3.8.0" } }, "nbformat": 4, "nbformat_minor": 4}